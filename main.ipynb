{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this about\n",
    "\n",
    "This is an experimentation in implementing Gaussian Discriminant Analyis for prediction milti-class classifications.\n",
    "\n",
    "### Scope\n",
    "\n",
    "The target of this study is to test the capability of Gaussian Discriminant Analysis in predicting the weather using a pre-defined dataset found at https://www.kaggle.com/datasets/nikhil7280/weather-type-classification\n",
    "\n",
    "### Approach\n",
    "\n",
    "This study will makes use of the following techniques and formulaes after derivations done by the researcher\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "P(y=k|x) = \\frac{1}{\\sum_{i=0, i\\not= k}^kexp(-\\theta^T_ix + \\theta_{0i}) } \n",
    "$$\n",
    "\n",
    "where, when predicting for $k$ \n",
    "\n",
    "$\\theta_i = \\Sigma^-(\\mu_k - \\mu_i)$ and $\\theta_{0i} = ln\\frac{\\phi_i}{\\phi_k} + \\frac{1}{2}(\\mu_k^T\\Sigma^-\\mu_k - \\mu_i^T\\Sigma^-\\mu_i)$\n",
    "\n",
    "Notice that it looks similar to how we make predictions in softmax\n",
    "\n",
    "### Matrix shortcut for computing the parameters\n",
    "\n",
    "We can easily compute for the parameters using the following matrix multipications \n",
    "\n",
    "$$\n",
    "\\phi = \\frac{1}{n}\\sum_{i=1}^nY\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{X^TY}{\\sum_{i=1}^nY}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n}\\sum_{i = 1}^n(x^i - \\mu_{y^i})(x^i - \\mu_{y^i})^T\n",
    "$$\n",
    "\n",
    "Where \n",
    "\n",
    " $\\phi\\exists\\R^{1\\times K}$\n",
    "\n",
    " $\\mu\\exists\\R^{D \\times K}$ = where the $h$ column represents the mean for the $i'th$ classification\n",
    "\n",
    "$X\\exists\\R^{M \\times D}$ = where the $i'th$ row represents the $i'th$ training data\n",
    "\n",
    "$Y\\exists\\R^{M \\times K}$ = where the $i'th$ row represents the classification of the $i'th$ data represented in a                     one-hot row matrix\n",
    "\n",
    "Using this we can solve for the $\\theta_i$  and $\\theta_{0i}$ \n",
    "\n",
    "### Matrix Shortcut For Predictions in Multi-class\n",
    "\n",
    "To easily compute $P(y = k|x)$  for every $k$ what we can do is perform the following matrix transformations\n",
    "\n",
    "$$\n",
    "H(k) = \\frac{1}{exp(-sum(X'W))}\n",
    "$$\n",
    "\n",
    "Where  \n",
    "\n",
    "$X'\\exists\\R^{1\\times D + 1}$ = Which is the set of features we want to make predictions for\n",
    "\n",
    "$W\\exists\\R^{D + 1\\times K}$ = Where the $i'th$ column represents the $\\theta_{i}$ for classification $k$ and the last row represents all the $\\theta_0$ where the $j'th$ column represents $\\theta_{0j}$\n",
    "\n",
    "$D$ = number of dimensions (features)\n",
    "\n",
    "$M$ = number of training data\n",
    "\n",
    "$K$  = number of classifications\n",
    "\n",
    "We then do this for every $k$ a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE 1: DATA SANITATION AND COLLECTION\n",
    "\n",
    "First we will get the data (X and Y) and compose it to our intended format.\n",
    "\n",
    "We also have to take into account, and assign number for the categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      " [[14.  73.   9.5 ...  2.   3.5  0. ]\n",
      " [39.  96.   8.5 ...  0.  10.   0. ]\n",
      " [30.  64.   7.  ...  0.   5.5  1. ]\n",
      " ...\n",
      " [30.  77.   5.5 ...  1.   9.   2. ]\n",
      " [ 3.  76.  10.  ...  2.   2.   0. ]\n",
      " [-5.  38.   0.  ...  1.  10.   1. ]]\n",
      "Y: \n",
      " [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\ron\\AppData\\Local\\Temp\\ipykernel_6724\\2459538622.py:14: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  file_path = 'dataset\\weather_classification_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# assign numbers to the categorical values\n",
    "categorical_mapping = {\n",
    "    \"Weather Type\": {\"Rainy\": 0, \"Cloudy\": 1, \"Sunny\": 2, \"Snowy\": 3},\n",
    "    \"Cloud Cover\" : {\"overcast\": 0.,\"partly cloudy\": 1., \"clear\": 2., \"cloudy\": 3.},\n",
    "    \"Season\"     : {\"Spring\": 0., \"Autumn\": 1., \"Winter\": 2., \"Summer\": 3.},\n",
    "    \"Location\"    : {\"inland\": 0., \"mountain\": 1., \"coastal\": 2.}\n",
    "}\n",
    "\n",
    "# configs\n",
    "classifications = [\"Rainy\", \"Cloudy\", \"Sunny\", \"Snowy\"]\n",
    "file_path = 'dataset\\weather_classification_data.csv'\n",
    "features = [\"Temperature\", \"Humidity\", \"Wind Speed\", \n",
    "            \"Precipitation (%)\", \"Cloud Cover\", \"Atmospheric Pressure\", \n",
    "            \"UV Index\", \"Season\", \"Visibility (km)\", \"Location\"]\n",
    "\n",
    "dependent_variable = \"Weather Type\"\n",
    "K = len(categorical_mapping[dependent_variable])\n",
    "D = len(features)\n",
    "M = None\n",
    "\n",
    "def display_tensor(tensor, name):\n",
    "    print(name + \": \\n\", tensor.numpy())\n",
    "\n",
    "def encode_row_matrix_to_one_hot(row_matrix, depth=K):\n",
    "    row_matrix = tf.reshape(row_matrix, [-1])\n",
    "    one_hot_matrix = tf.one_hot(row_matrix, depth=depth)\n",
    "    \n",
    "    return one_hot_matrix\n",
    "\n",
    "def read_csv_to_XYy(file_path, categorical_mapping, dependent_variable):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    for column, mapping in categorical_mapping.items():\n",
    "        df[column] = df[column].map(mapping)\n",
    "\n",
    "    y = df[dependent_variable]\n",
    "    x = df.drop(columns = [dependent_variable])\n",
    "\n",
    "    X = tf.convert_to_tensor(x.values, dtype=tf.float32)\n",
    "    Y = encode_row_matrix_to_one_hot(tf.convert_to_tensor(y.values, dtype=tf.uint8))\n",
    "\n",
    "    return [X, Y, y.values]\n",
    "\n",
    "# parse the data to X and Y\n",
    "X, Y, y = read_csv_to_XYy(file_path, categorical_mapping, dependent_variable)\n",
    "X_T = tf.transpose(X)\n",
    "M = len(X)\n",
    "\n",
    "display_tensor(X, \"X\")\n",
    "display_tensor(Y, \"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE 2: COMPUTE FOR THE PARAMTERS\n",
    "We then compute for our intended parameters which are $\\phi, \\Sigma, \\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHI: \n",
      " [0.25 0.25 0.25 0.25]\n",
      "MU: \n",
      " [[ 2.27881813e+01  2.28236370e+01  3.24290924e+01 -1.53060603e+00]\n",
      " [ 7.83978806e+01  6.65287857e+01  5.14063644e+01  7.85102997e+01]\n",
      " [ 1.36775761e+01  8.60181808e+00  6.07318163e+00  1.09762125e+01]\n",
      " [ 7.47524261e+01  4.02863655e+01  2.49527264e+01  7.45860596e+01]\n",
      " [ 3.99090916e-01  6.60303056e-01  1.68181813e+00  3.10606062e-01]\n",
      " [ 1.00414972e+03  1.01017078e+03  1.01793933e+03  9.91051697e+02]\n",
      " [ 2.68424249e+00  3.58393931e+00  7.80454540e+00  1.95030308e+00]\n",
      " [ 1.50363636e+00  1.47272730e+00  1.49636364e+00  1.95030308e+00]\n",
      " [ 3.62848496e+00  7.07121229e+00  7.56045437e+00  3.59151506e+00]\n",
      " [ 1.04454541e+00  9.99696970e-01  1.01939392e+00  5.59090912e-01]]\n",
      "SIGMA: \n",
      " [[ 1.4457471e+02  2.8291887e+01  7.6160531e+00  3.7079231e+01\n",
      "   1.6194342e-01  1.6221891e+01  3.4787211e+00 -1.7164505e-01\n",
      "  -1.7499726e+00  2.2594477e-01]\n",
      " [ 2.8291887e+01  2.8425623e+02  2.7601900e+01  1.7813998e+02\n",
      "  -1.6614841e+00  4.3973594e+00 -2.2474716e+00  6.0635775e-01\n",
      "  -1.3697409e+01 -4.1723189e-01]\n",
      " [ 7.6160531e+00  2.7601900e+01  3.9791225e+01  4.0583275e+01\n",
      "  -2.2784054e-01 -1.4254702e+00  3.4829023e+00  1.3255647e-01\n",
      "  -2.4981315e+00 -7.0704274e-02]\n",
      " [ 3.7079231e+01  1.7813998e+02  4.0583275e+01  5.4906311e+02\n",
      "  -9.1888040e-01 -2.3268381e+01  7.6505327e+00  3.8472086e-01\n",
      "  -9.3722754e+00 -3.1913394e-01]\n",
      " [ 1.6194342e-01 -1.6614841e+00 -2.2784054e-01 -9.1888040e-01\n",
      "   3.9377993e-01  1.8942414e-01  3.4153104e-01 -2.2518251e-02\n",
      "   3.2987261e-01  1.7976271e-02]\n",
      " [ 1.6221891e+01  4.3973594e+00 -1.4254702e+00 -2.3268381e+01\n",
      "   1.8942414e-01  1.2870317e+03  2.9188495e+00 -4.4243997e-01\n",
      "  -7.0770693e-01  1.7233334e-01]\n",
      " [ 3.4787211e+00 -2.2474716e+00  3.4829023e+00  7.6505327e+00\n",
      "   3.4153104e-01  2.9188495e+00  9.7271605e+00 -6.1146889e-02\n",
      "   1.3284169e+00  4.6227649e-02]\n",
      " [-1.7164505e-01  6.0635775e-01  1.3255647e-01  3.8472086e-01\n",
      "  -2.2518251e-02 -4.4243997e-01 -6.1146889e-02  9.7030711e-01\n",
      "  -1.0482296e-01 -5.2754846e-03]\n",
      " [-1.7499726e+00 -1.3697409e+01 -2.4981315e+00 -9.3722754e+00\n",
      "   3.2987261e-01 -7.0770693e-01  1.3284169e+00 -1.0482296e-01\n",
      "   7.9028459e+00  8.3217457e-02]\n",
      " [ 2.2594477e-01 -4.1723189e-01 -7.0704274e-02 -3.1913394e-01\n",
      "   1.7976271e-02  1.7233334e-01  4.6227649e-02 -5.2754846e-03\n",
      "   8.3217457e-02  5.8619928e-01]]\n",
      "SIGMA_INV: \n",
      " [[ 7.19545595e-03 -5.18150453e-04 -5.62313013e-04 -2.45580304e-04\n",
      "  -4.55870200e-03 -8.69109062e-05 -2.20147264e-03  1.55969663e-03\n",
      "   8.32246675e-04 -3.10889073e-03]\n",
      " [-5.18150511e-04  4.92915977e-03 -1.69178750e-03 -1.35364279e-03\n",
      "   1.05132777e-02 -4.01284051e-05  1.87052751e-03 -1.45854848e-03\n",
      "   5.49632823e-03  1.51558779e-03]\n",
      " [-5.62312955e-04 -1.69178750e-03  2.91256458e-02 -1.29047292e-03\n",
      "   1.11452769e-02  4.70311461e-05 -1.08407205e-02 -2.26344448e-03\n",
      "   5.93544869e-03  1.45934930e-03]\n",
      " [-2.45580275e-04 -1.35364302e-03 -1.29047304e-03  2.40521645e-03\n",
      "   4.49007493e-04  5.37596170e-05 -1.73155603e-03 -1.29166692e-05\n",
      "   3.17273923e-04  3.46789253e-04]\n",
      " [-4.55870200e-03  1.05132759e-02  1.11452779e-02  4.49008105e-04\n",
      "   2.73700643e+00 -1.88989899e-04 -8.52207094e-02  4.03078981e-02\n",
      "  -7.75559172e-02 -5.49550205e-02]\n",
      " [-8.69109062e-05 -4.01284160e-05  4.70311497e-05  5.37596243e-05\n",
      "  -1.88989987e-04  7.80099595e-04 -2.78330699e-04  3.27813352e-04\n",
      "   1.20558507e-04 -1.75879497e-04]\n",
      " [-2.20147241e-03  1.87052705e-03 -1.08407205e-02 -1.73155603e-03\n",
      "  -8.52207094e-02 -2.78330728e-04  1.14900440e-01  3.73501400e-03\n",
      "  -1.84180792e-02 -3.78792593e-03]\n",
      " [ 1.55969651e-03 -1.45854813e-03 -2.26344378e-03 -1.29166447e-05\n",
      "   4.03078906e-02  3.27813323e-04  3.73501354e-03  1.03436422e+00\n",
      "   8.47735815e-03  4.55894927e-03]\n",
      " [ 8.32246791e-04  5.49632870e-03  5.93544822e-03  3.17274244e-04\n",
      "  -7.75559098e-02  1.20558514e-04 -1.84180792e-02  8.47735815e-03\n",
      "   1.45085230e-01 -1.22449324e-02]\n",
      " [-3.10889073e-03  1.51558791e-03  1.45934918e-03  3.46789195e-04\n",
      "  -5.49550205e-02 -1.75879482e-04 -3.78792500e-03  4.55894880e-03\n",
      "  -1.22449342e-02  1.71236110e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_SUM = tf.reduce_sum(Y, axis=0)\n",
    "PHI = Y_SUM / M\n",
    "MU  = tf.matmul(X_T, Y) / Y_SUM\n",
    "SIGMA = tf.Variable(initial_value=tf.zeros(shape=(D, D)), dtype=tf.float32)\n",
    "\n",
    "# Sigma\n",
    "for i in range(M):\n",
    "    diff = tf.transpose([X_T[:, i] - MU[:, y[i]]])\n",
    "    SIGMA.assign(SIGMA + (tf.matmul(diff, tf.transpose(diff)) / M))\n",
    "\n",
    "SIGMA_INV = tf.linalg.inv(SIGMA)\n",
    "\n",
    "display_tensor(PHI, \"PHI\")\n",
    "display_tensor(MU, \"MU\")\n",
    "display_tensor(SIGMA, \"SIGMA\")\n",
    "display_tensor(SIGMA_INV, \"SIGMA_INV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE 3: COMPUTING FOR $\\theta_1i$ and $\\theta_0i$\n",
    "\n",
    "With these parameters in hand, we can then compute for the $\\theta$ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA_LIST = []\n",
    "\n",
    "# mu^TSigma^-mu\n",
    "mu_t_sigma_inv_mu = []\n",
    "for i in range(K):\n",
    "    MU_COL = tf.expand_dims(MU[:, i], axis=1)\n",
    "    RESULT = tf.matmul(tf.transpose(MU_COL), tf.matmul(SIGMA_INV, MU_COL))\n",
    "    mu_t_sigma_inv_mu.append(RESULT)\n",
    "\n",
    "mu_t_sigma_inv_mu = tf.reshape(tf.convert_to_tensor(mu_t_sigma_inv_mu, dtype=tf.float32), [-1])\n",
    "\n",
    "# compute for theta_1i and theta_0i, combine then and append them to THETA_LIST\n",
    "for i in range(K):\n",
    "    THETA_1 = tf.matmul(SIGMA_INV, tf.subtract(tf.tile(tf.expand_dims(MU[:, i], axis=1), [1, K]), MU)) * -1\n",
    "    THETA_0 = tf.expand_dims(tf.math.log(PHI / PHI[i]) + 0.5 * tf.subtract(mu_t_sigma_inv_mu[i], mu_t_sigma_inv_mu), axis=0)\n",
    "    THETA_LIST.append(tf.concat([THETA_1, THETA_0], axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE 4: PREDICTION AND TESTING \n",
    "Now that we have all the parameters, we're ready to create predictions. We are also ready to check the accuracy or our model using Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY ACCROSS ALL DATA:  tf.Tensor(75.15319901837556, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "bias = tf.constant([1], dtype=tf.float32)\n",
    "\n",
    "def H(row_tensor_feature):\n",
    "    prediction = []\n",
    "\n",
    "    # add bias in the end for the intercept\n",
    "    expanded = tf.expand_dims(tf.concat([row_tensor_feature, bias], axis=0), axis=0)\n",
    "    for i in range(K):\n",
    "        XW = tf.matmul(expanded, THETA_LIST[i])\n",
    "        prediction.append(1 / tf.reduce_sum(tf.exp(XW)))\n",
    "    \n",
    "    return tf.convert_to_tensor(prediction, dtype=tf.float64)\n",
    "\n",
    "def calculate_accuracy():\n",
    "    accuracy = 0\n",
    "    \n",
    "    for i in range(M):\n",
    "        result = H(X[i])\n",
    "        accuracy += result[y[i]] / M\n",
    "\n",
    "    return accuracy * 100\n",
    "\n",
    "print(\"ACCURACY ACCROSS ALL DATA: \", calculate_accuracy().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
